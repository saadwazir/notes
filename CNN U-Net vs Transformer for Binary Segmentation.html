<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>CNN U-Net vs Transformer for Binary Segmentation</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 20px;
      background: #f9f9f9;
      color: #333;
    }
    h2 {
      margin-top: 40px;
      color: #222;
      border-bottom: 2px solid #ccc;
      padding-bottom: 5px;
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
      background: #fff;
    }
    th, td {
      border: 1px solid #ccc;
      padding: 10px;
      text-align: left;
      vertical-align: top;
    }
    th {
      background: #f0f0f0;
    }
    tr:nth-child(even) {
      background: #fafafa;
    }
  </style>
</head>
<body>

<h1>CNN U-Net vs Transformer Encoder–Decoder (Binary Segmentation)</h1>

<table>
  <tr>
    <th>Stage</th>
    <th>U-Net Layer</th>
    <th>Feature Dim (U-Net)</th>
    <th>Role</th>
    <th>Transformer Layer</th>
    <th>Feature Dim (Transformer)</th>
    <th>Role</th>
  </tr>

  <tr>
    <td>Input</td>
    <td>Raw Image</td>
    <td>256×256×3</td>
    <td>Input RGB image</td>
    <td>Image split into 16×16 patches</td>
    <td>256 tokens × 768 dims</td>
    <td>Convert image into tokens</td>
  </tr>

  <tr>
    <td>Encoder – Stage 1</td>
    <td>Conv(3×3,64) → Conv(3×3,64), MaxPool</td>
    <td>128×128×64</td>
    <td>Extract low-level edges/textures</td>
    <td>Linear patch embedding</td>
    <td>256×768</td>
    <td>Represent patches as embeddings</td>
  </tr>

  <tr>
    <td>Encoder – Stage 2</td>
    <td>Conv(128) + Pool</td>
    <td>64×64×128</td>
    <td>Mid-level features</td>
    <td>Transformer encoder block</td>
    <td>256×768</td>
    <td>Global context via self-attention</td>
  </tr>

  <tr>
    <td>Encoder – Stage 3</td>
    <td>Conv(256) + Pool</td>
    <td>32×32×256</td>
    <td>Higher-level structures</td>
    <td>Deeper Transformer encoders</td>
    <td>256×768</td>
    <td>Capture long-range dependencies</td>
  </tr>

  <tr>
    <td>Encoder – Stage 4</td>
    <td>Conv(512) + Pool</td>
    <td>16×16×512</td>
    <td>Abstract semantics</td>
    <td>Transformer latent tokens</td>
    <td>256×768</td>
    <td>Global semantic representation</td>
  </tr>

  <tr>
    <td>Bottleneck</td>
    <td>Conv(1024)</td>
    <td>16×16×1024</td>
    <td>Compact latent features</td>
    <td>Transformer token sequence</td>
    <td>256×768</td>
    <td>Final latent representation</td>
  </tr>

  <tr>
    <td>Skip Connections</td>
    <td>Feature maps copied to decoder</td>
    <td>128×128, 64×64, 32×32, 16×16</td>
    <td>Preserve spatial detail</td>
    <td>Token–feature fusion</td>
    <td>Mixed (tokens + feature maps)</td>
    <td>Inject local detail back</td>
  </tr>

  <tr>
    <td>Decoder – Stage 1</td>
    <td>UpConv(512) + Concat(encoder)</td>
    <td>32×32×512</td>
    <td>Reconstruct with skip info</td>
    <td>Decoder cross-attention</td>
    <td>Reshape tokens → 32×32×C</td>
    <td>Decode tokens into spatial map</td>
  </tr>

  <tr>
    <td>Decoder – Stage 2</td>
    <td>UpConv(256) + Concat</td>
    <td>64×64×256</td>
    <td>Refine spatial resolution</td>
    <td>Upsampling + projection</td>
    <td>64×64×C</td>
    <td>Reconstruct mid-level features</td>
  </tr>

  <tr>
    <td>Decoder – Stage 3</td>
    <td>UpConv(128) + Concat</td>
    <td>128×128×128</td>
    <td>Sharpen object boundaries</td>
    <td>Progressive reconstruction</td>
    <td>128×128×C</td>
    <td>Refine mask details</td>
  </tr>

  <tr>
    <td>Decoder – Final</td>
    <td>UpConv(64) + Concat → Conv(1×1,1)</td>
    <td>256×256×1</td>
    <td>Output binary mask</td>
    <td>Projection head + Sigmoid</td>
    <td>256×256×1</td>
    <td>Output binary mask</td>
  </tr>

  <tr>
    <td>Feature Extraction</td>
    <td>Convolutions with local receptive fields</td>
    <td>Hierarchical spatial maps</td>
    <td>Local texture + shape features</td>
    <td>Self-attention with global receptive field</td>
    <td>Token sequence preserved</td>
    <td>Global dependencies captured</td>
  </tr>

  <tr>
    <td>Mask Generation</td>
    <td>Upsampling with transposed convs</td>
    <td>Pixel-level spatial maps</td>
    <td>Reconstruct detailed mask</td>
    <td>Linear projection of decoded tokens</td>
    <td>Pixel grid restored</td>
    <td>Reconstruct binary mask</td>
  </tr>
</table>

</body>
</html>
